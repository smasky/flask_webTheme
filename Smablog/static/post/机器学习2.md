@-------------------------
title:: 机器学习-----支持向量机原理（二）
top_img:: "https://ws1.sinaimg.cn/large/e4e313f5gy1fyw5r9kiscj21hc0u0dpc.jpg"
abstract:: 不积跬步无以至千里
@-------------------------
# 机器学习-----支持向量机原理（二）

## 一、SMO简介

​	序列最小优化算法（Sequential minimal optimization），简称SMO，是一种用于解决支持向量机训练过程中所产生的优化问题的算法。SMO由微软研究院的Platt于1998年发明，目前广泛的用于SVM的训练过程中。SMO算法的核心思想就是将一个庞大的优化问题的分隔成一个个小的优化问题，在一堆变量中，两两优化，比起先前SVM的优化算法，极大的缩短了计算的时间以及计算量。SMO算法发表在SVM研究领域内引起了极大的轰动。

## 二、SVM在SMO算法下求解

​	接下来，我们来讨论一下之前化简的硬间隔下的SVM优化如何使用SMO算法来求解。回顾一下上一章留下的优化问题。

​	问题如下：

<p style='font-size:30px'>$L(\alpha)=-\frac{1}{2}\sum\limits_{j=1}^{n}\alpha_jy_j\sum\limits_{i=1}^{n}\alpha_iy_ix_ix_j+\sum\limits_{i=1}^{n}\alpha_i$</p>

<p style='font-size:30px'>$\max\limits_{\alpha_i \ge 0}L(\alpha)$</p>

<p style='font-size:30px'>$s.t.\alpha_i\ge0$</p>

<p style='font-size:30px'>$y_i(w^Tx+b)\ge1$</p>

<p style='font-size:30px'>$\alpha_i g(x_i)=0,i=0,1,2,3...n$</p>

​	首先我们需要明白的一点是现在的问题就是求出最优的$\alpha_i$就可以了。$w、b$均可由$\alpha_i$来求出来。先前我们已经推导到：

<p style='font-size:30px'>$w=\sum\limits_{i=1}^{n}\alpha_iy_ix_i$</p>

​	这几点要非常明确，不然到最后都不知道我们的优化目标了，容易在一大堆参数中迷失自己。

​	SMO算法就是一次选取$\alpha_i$中的两个参数来进行优化,逐步收敛到最优解上。

​	假设我们选取$\alpha_1$、$\alpha_2$作为一次迭代中的选取两个参数。其余$\alpha_i$均看作为常量。就可以得到下面这个式子：

<p style='font-size:30px'>$\alpha_1y_1+\alpha_2y_2=\sum\limits_{i=3}^{n}\alpha_i=\zeta$</p>

​	化简一下：

<p style='font-size:30px'>$\alpha_1=y_1\zeta-y_1y_2\alpha_2$</p>

​	令$\gamma=y_1\zeta、s=y_1y_2$:

<p style='font-size:30px'>$\alpha_1=\gamma-s\alpha_2$</p>

​	所以把$\alpha_1=\gamma-s\alpha_2$带入$L(\alpha)$，得到有关于$\alpha_1$的方程，这边推导比较复杂，要慢慢来：

​	先把自变量分离开：

<p style='font-size:30px'>$L(\alpha)=-\frac{1}{2}\sum\limits_{j=3}^{n}\alpha_jy_j\sum\limits_{i=3}^{n}\alpha_iy_ix_ix_j+\sum\limits_{i=3}^{n}\alpha_i+W(\alpha_1,\alpha_2)$</p>

<p style='font-size:15px'>$W(\alpha_1,\alpha_2)=-\frac{1}{2}(\alpha_1y_1x_1)^2-(\alpha_1\alpha_2x_1x_2y_1y_2)-\frac{1}{2}(\alpha_2y_2x_2)^2-(\alpha_1y_1x_1+\alpha_2y_2x_2)\sum\limits_{j=3}^{n}\alpha_jy_jx_y+\alpha_1+\alpha_2$</p>

<p style='font-size:15px'>$f(x_i)=\sum\limits_{j=1}^{n}\alpha_jy_jx_yx_i+b$</p>

<p style='font-size:15px'>$v_i=f(x_i)-\sum\limits_{j=1}^{2}\alpha_jy_jx_jx_i-b$</p>

​	可能有人会问为什么$f(x_i)$假设成这个啊，因为$w=\sum\limits_{i=1}^{n}\alpha_iy_ix_i$啊。然后将$f(x_i)、v_i$代入到$W$中：

<p style='font-size:15px'>$W(\alpha_1,\alpha_2)=-\frac{1}{2}(\alpha_1y_1x_1)^2-(\alpha_1\alpha_2x_1x_2y_1y_2)-\frac{1}{2}(\alpha_2y_2x_2)^2-y_1\alpha_1v_1-y_2\alpha_2v_2+\alpha_1+\alpha_2$</p>

<p style='font-size:30px'>$Const=-\frac{1}{2}\sum\limits_{j=3}^{n}\alpha_jy_j\sum\limits_{i=3}^{n}\alpha_iy_ix_ix_j+\sum\limits_{i=3}^{n}\alpha_i$</p>

<p style='font-size:30px'>$L(\alpha)=W(\alpha_1,\alpha_2)+Const$</p>

​	然后将$\alpha_1=\gamma-s\alpha_2$代入W：

<p style='font-size:15px'>$W(\alpha_2)=(\gamma-s\alpha_2)+\alpha_2-(\gamma-s\alpha_2)\alpha_2x_1x_2y_1y_2-\frac{1}{2}(\alpha_2y_2x_2)^2-\frac{1}{2}((\gamma-s\alpha_2)y_1x_1)^2-y_2\alpha_2v_2-(\gamma-s\alpha_2)y_1v_1$</p>

​	然后将$L(\alpha_2)$对$\alpha_2$求导：

<p style='font-size:20px'>$\frac{\partial W(\alpha_2)}{\partial \alpha_2}=-s+1+2\alpha_2x_1x_2-\alpha_2x_1x_1-\alpha_2x_2x_2+y_2v_1-y_2v_2+rsx_1x_1-rsx_1x_2$</p>

​	令$\frac{\partial W（\alpha_2）}{\alpha_2}=0$即为针对$\alpha_1$、$\alpha_2$的最优解。

<p style='font-size:30px'>$\alpha_2=\frac{y_2(y_2-y_1+y_1r(x_1x_1-x_1x_2)+v_1-v_2)}{x_1x_1+x_2x_2-2x_1x_2}$</p>

​	将$r=\alpha_1+s\alpha_2$代入方程：

<p style='font-size:30px'>$\alpha_2=\frac{v_1+\alpha_1y_1x_1x_1+\alpha_2y_2x_2x_1-(v_2+\alpha_1y_1x_1x_2+\alpha_2y_2x_2x_2)+\alpha_2(x_1x_1+x_2x_2-2x_1x_2)}{x_1x_1+x_2x_2-2x_1x_2}$</p>

​	令$E_i=v_i+\sum\limits_{j=1}^{2}\alpha_jx_jy_jx_i-b$、$\eta=x_1x_1+x_2x_2-2x_1x_2$得到：

<p style='font-size:30px'>$\alpha_2^{new}=\alpha_2^{old}+\frac{E_1-E_2}{\eta}$</p>

<p style='font-size:30px'>$\alpha_1^{new}=s\alpha_2^{old}+\alpha_1^{old}-s\alpha_2^{new}=\zeta-s\alpha_2^{new}$</p>

​	这边用到了迭代法。我们通过原来的$\alpha_2^{old}$就可以求出$\alpha_2^{new}$。通过$\alpha_2^{new}$就可以求得$\alpha_2^{old}$。但是我们要注意的一点是，目前在整个推演过程中还有两个约束没有使用。

<p style='font-size:30px'>$\alpha_i\ge0$</p>

<p style='font-size:30px'>$y_i(w^Tx+b)\ge1$</p>

​	那也就是说新迭代产生的$\alpha_1、\alpha_2$要满足这两个约束条件。我们先来看第一个约束条件。这个地方有点像我们初中学习的线性规划的约束条件。

​	分两种情况，第一种$y_1、y_2$是异号，即$s=1$。

<p style='font-size:30px'>$\alpha_2^{new}=-\zeta+\alpha_1^{new}$</p>

​	可把$\alpha_2^{new}$看成是因变量，$\alpha_1^{new}$看成自变量，也就是$\alpha_1^{new}、\alpha_2^{new}$满足以下这三个条件:

<p style='font-size:30px'>$\alpha_2^{new}=-\zeta+\alpha_1^{new}$</p>

<p style='font-size:30px'>$\alpha_1\ge0$</p>

<p style='font-size:30px'>$\alpha_2\ge0$</p>

​	见下图所示：

​	![](https://ws1.sinaimg.cn/large/e4e313f5gy1fyw2txvvohj20fo0e6dha.jpg)

这三个约束在坐标图上的位置关系有两种关系，分别为1,2两种情况。

也就是说$\alpha_2$必须满足以下这个情况：

<p style='font-size:30px'>$\alpha_2^{new}\ge max(0,-\zeta)$</p>

第二种情况$y_1、y_2$是异号，即$s=$-1。

也是要满足三个约束条件，情况见下图所示。

![](https://ws1.sinaimg.cn/large/e4e313f5gy1fyw3fgwqtuj20go0giglv.jpg)

情况就这一种，其他直线位置都不满足条件，也就是说:

<p style='font-size:30px'>$0 \ge \alpha_2^{new}\ge \zeta$</p>

可能会有同学问，前文说个有三个约束条件没有用，还有一个$y_i(w^Tx+b)\ge1$没有用啊？

这个约束就是用来选择合适的$\alpha_1、\alpha_2$来迭代。我们再来考虑一下这个约束。

<p style='font-size:30px'>$\alpha_i g(x_i)=0,i=0,1,2,3...n$</p>

要使上面的等式成立，必须满足以下两种情况之一：

<p style='font-size:30px'>$\alpha_i> 0、y_i(w^Tx+b)=1$</p>

<p style='font-size:30px'>$\alpha_i=0、y_i(w^Tx+b)>1$</p>

这也是我们选择$\alpha_i$的一个条件，即选择那些不满足以上两种情况的$\alpha_i$，即

<p style='font-size:30px'>$\alpha_i> 0、y_i(w^Tx+b)\ne 1$</p>

<p style='font-size:30px'>$\alpha_i=0、y_i(w^Tx+b)\le 1$</p>

即若不存在这样的$\alpha_i$时，$L(\alpha_i)$达到最优解。

## 三、SMO算法流程

​	下面介绍一下SMO算法的流程。

准备工作：给定$\alpha_i$初值，可令$\alpha_i$=0，b=0。

1.寻找一个合适的$\alpha_2$来进行迭代，利用遍历或者其他方法，寻找不满足以下条件的$\alpha_i$

<p style='font-size:30px'>$\alpha_i> 0、y_i(w^Tx+b)\ne 1$</p>

<p style='font-size:30px'>$\alpha_i=0、y_i(w^Tx+b)\le 1$</p>

注:$w^T=\sum\limits_{j=1}^{n}\alpha_ix_iy_i$

2.确定下$\alpha_2$，遍历$\alpha_i$，选取$|E_i-E_2|$最大的作为$\alpha_1$,这样的目的是可以最大程度的改变$\alpha_2$，提升迭代的速率。

3.由公式$\alpha_2^{new}=\alpha_2^{old}+\frac{E_1-E_2}{\eta}$，计算$\alpha_2^{new}$，并且需满足$\alpha_2^{new}$的约束条件，超过就取边界值。

4.由$\alpha_2^{new}$计算$\alpha_1^{new}$，其次计算$w^T,b$。

5.验证是否所有$\alpha_i$满足约束条件，不满足的话，继续步骤2，满足的话，输出。

## 四、结语

​	根据网上SVM的介绍，几乎没有介绍硬间隔下的SVM。大部分是前面介绍的硬间隔，到SMO的算法，莫名其妙给$\alpha_i$加了一个限制条件$c$。很容易让初学者一头雾水，所以我借的大佬的肩膀，给大家解释一下，我在学习机器学习的过程的一些自己的看法，尽量完整的把硬间隔下的SVM给讲清楚，接下来将会介绍软间隔下的SVM。另外硬间隔SVM在我的github会有算例，过几天在整理。
